{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77744bf1-16f4-4e99-b9c6-29c926b94b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b02cdfc-53ff-4f7e-9614-c83a5f8a5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055b7e16-6bd9-4769-9352-3fb72fdf703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2585bb1f-688c-4a8c-8ea4-010ded7bc3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f1ee47abef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3973f3-beeb-40bc-9cac-8d4bd3b27e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18799298-39de-4ba8-bf15-5311f672e300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964aa9fe-b045-4b65-8e34-ceebfda75cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I want you to be my tutor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Of course! I'd be happy to help you learn. What subject or topic would you like assistance with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  can you teach me how to write effective CUDa kernel for ML applications\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Sure! Writing effective CUDA kernels for machine learning applications involves several considerations to maximize performance on NVIDIA GPUs. Here’s a guide to get you started:\n",
      "\n",
      "### Basics of CUDA Programming\n",
      "\n",
      "1. **Understanding CUDA Architecture**:\n",
      "   - Familiarize yourself with the GPU architecture, including threads, blocks, and grids.\n",
      "   - CUDA programs are structured into host (CPU) code and device (GPU) code.\n",
      "\n",
      "2. **CUDA Kernels**:\n",
      "   - A kernel is a function that runs on the GPU and is invoked from the CPU. You define a kernel with the `__global__` qualifier.\n",
      "\n",
      "### Writing Efficient CUDA Kernels\n",
      "\n",
      "1. **Memory Management**:\n",
      "   - **Global Memory**: Access speed is slower; minimize accesses by coalescing memory operations.\n",
      "   - **Shared Memory**: Fast access and shared among threads in the same block. Use it to store frequently accessed data.\n",
      "   - **Registers**: Fastest memory; avoid excessive usage as it can limit the number of active threads.\n",
      "\n",
      "2. **Thread Organization**:\n",
      "   - Organize threads in blocks and grids. A common approach is to use 1D, 2D, or 3D thread block layouts depending on the problem.\n",
      "   - Choose an appropriate block size. A typical choice is a multiple of 32 (the warp size) to fully utilize the GPU.\n",
      "\n",
      "3. **Optimizing Kernel Execution**:\n",
      "   - Minimize divergent branches within warp execution.\n",
      "   - Use loop unrolling techniques for performance.\n",
      "   - Avoid bank conflicts in shared memory.\n",
      "\n",
      "4. **Using Libraries**:\n",
      "   - Leverage libraries such as cuBLAS, cuDNN, or Thrust for optimized routines that are well-tested and highly optimized.\n",
      "\n",
      "### Example: Simple Matrix Multiplication\n",
      "\n",
      "Here's a basic example of a CUDA kernel for matrix multiplication, which is a common operation in ML.\n",
      "\n",
      "```cpp\n",
      "#include <stdio.h>\n",
      "\n",
      "#define N 16  // Size of the matrices\n",
      "\n",
      "__global__ void matrixMulKernel(float *C, float *A, float *B, int n) {\n",
      "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "\n",
      "    if (row < n && col < n) {\n",
      "        float sum = 0;\n",
      "        for (int k = 0; k < n; k++) {\n",
      "            sum += A[row * n + k] * B[k * n + col];\n",
      "        }\n",
      "        C[row * n + col] = sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "void matrixMultiply(float *C, float *A, float *B, int n) {\n",
      "    float *d_A, *d_B, *d_C;\n",
      "    size_t size = n * n * sizeof(float);\n",
      "\n",
      "    // Allocate memory on the GPU\n",
      "    cudaMalloc(&d_A, size);\n",
      "    cudaMalloc(&d_B, size);\n",
      "    cudaMalloc(&d_C, size);\n",
      "\n",
      "    // Copy matrices from host to device\n",
      "    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);\n",
      "\n",
      "    // Define grid and block size\n",
      "    dim3 block(4, 4); // 4x4 threads per block\n",
      "    dim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n",
      "\n",
      "    // Launch the kernel\n",
      "    matrixMulKernel<<<grid, block>>>(d_C, d_A, d_B, n);\n",
      "\n",
      "    // Copy result back to host\n",
      "    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);\n",
      "\n",
      "    // Free GPU memory\n",
      "    cudaFree(d_A);\n",
      "    cudaFree(d_B);\n",
      "    cudaFree(d_C);\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    float A[N*N], B[N*N], C[N*N];\n",
      "    // Initialize matrices A and B here\n",
      "\n",
      "    // Call the matrix multiplication function\n",
      "    matrixMultiply(C, A, B, N);\n",
      "\n",
      "    // Use the resulting matrix C here\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "- **Profiling**: Use profiling tools like NVIDIA Nsight Compute to analyze the performance of your kernels, identify bottlenecks, and iteratively refine your code.\n",
      "- **Optimizations**: Focus on memory access patterns, minimize global memory accesses, and experiment with block sizes.\n",
      "- **Debugging**: Utilize `cudaMemcpy` assertions and error checking after CUDA API calls to catch issues early.\n",
      "\n",
      "### Learning Resources\n",
      "\n",
      "- **Books**: \"CUDA by Example\" by Jason Sanders and Edward Kandrot, \"Programming Massively Parallel Processors\" by David B. Kirk and Wen-mei W. Hwu.\n",
      "- **Online Courses**: NVIDIA offers various online courses on CUDA and GPU programming.\n",
      "- **Documentation**: The official CUDA documentation is a great resource for functions, best practices, and examples.\n",
      "\n",
      "If you have specific applications or concepts you'd like to dive deeper into, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hmm how to use nvidia nsight, and waht is global memory why I shoudl minimize its access and how to expereiment with block size, what is memcpy assertion, also who is the best guy to refer to for doing this\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Let's break down your questions one by one to provide clarity on these concepts and tools related to CUDA programming.\n",
      "\n",
      "### 1. Nvidia Nsight\n",
      "\n",
      "**NVIDIA Nsight** is a powerful developer tool suite for debugging and profiling CUDA applications. Here’s how to get started with it:\n",
      "\n",
      "- **Installation**: Install the appropriate version of Nsight that corresponds with your CUDA toolkit. This is often included in the NVIDIA CUDA Toolkit installation.\n",
      "  \n",
      "- **Profiling a CUDA Application**:\n",
      "  1. Open your CUDA project in Visual Studio or your preferred IDE.\n",
      "  2. Go to `NVIDIA Nsight` in the menu and select `Start Graphics Debugger` or `Start CUDA Debugger`.\n",
      "  3. Choose the profiling option (including GPU utilization, memory bandwidth, etc.).\n",
      "  4. Run your application and capture performance data.\n",
      "  5. Analyze the captured data for bottlenecks such as memory access patterns, kernel launch latency, etc.\n",
      "\n",
      "- **Key Features**:\n",
      "  - Kernel profiling information.\n",
      "  - Memory access patterns and latency.\n",
      "  - Performance metrics (e.g., instruction throughput, memory access efficiency).\n",
      "  - Visualization of thread execution and memory bandwidth usage.\n",
      "\n",
      "### 2. Global Memory in CUDA\n",
      "\n",
      "**Global Memory** is the largest type of memory available in CUDA, shared among all threads in the grid. However, it's also the slowest. Here's why minimizing its access is crucial:\n",
      "\n",
      "- **Access Latency**: Accessing global memory can be significantly slower than accessing shared or local memory. If your kernel constantly reads from or writes to global memory, it can become a bottleneck.\n",
      "  \n",
      "- **Memory Coalescing**: If multiple threads access consecutive memory addresses (called coalescing), the access speed can increase. If threads access scattered memory locations, the performance can drop dramatically. \n",
      "\n",
      "**To Minimize Access**:\n",
      "- Use shared memory for frequently accessed data.\n",
      "- Structure your algorithms to reduce the number of global memory accesses.\n",
      "- Use appropriate data layouts (e.g., struct of arrays vs. array of structures) to enhance memory access patterns.\n",
      "\n",
      "### 3. Experimenting with Block Size\n",
      "\n",
      "Choosing the right block size is crucial for performance. Here’s how to experiment:\n",
      "\n",
      "- **Basic Guidelines**:\n",
      "  - Use a block size that is a multiple of the warp size (32 for NVIDIA GPUs right now).\n",
      "  - Common sizes include 32, 64, 128, and 256 threads per block.\n",
      "\n",
      "- **Experimentation**:\n",
      "  1. Start with a certain block size (e.g., 16x16 or 32x32 for 2D kernels).\n",
      "  2. Measure performance (execution time, memory throughput) using Nsight or simple timing mechanisms (like CUDA events).\n",
      "  3. Try varying block sizes and see how it affects execution time.\n",
      "  4. Optimize based on the metrics gathered, focusing on the execution context of your specific application.\n",
      "\n",
      "### 4. CUDA `memcpy` Assertions\n",
      "\n",
      "Using `cudaMemcpy` assertions helps ensure that the memory operations between host and device are successful. In C/C++, you can assert that the transfer completed without errors:\n",
      "\n",
      "```cpp\n",
      "cudaError_t err = cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);\n",
      "if (err != cudaSuccess) {\n",
      "    fprintf(stderr, \"Error copying to device: %s\\n\", cudaGetErrorString(err));\n",
      "    // handle error, exit, etc.\n",
      "}\n",
      "```\n",
      "\n",
      "This checks if the memory copy was successful. If it encounters an error, you get a debug message that can guide you to the source of the problem.\n",
      "\n",
      "### 5. Resources and References\n",
      "\n",
      "To deepen your understanding of CUDA programming and optimization strategies, here are some notable authors and resources:\n",
      "\n",
      "- **Books**:\n",
      "  - \"CUDA Programming: A Developer's Guide to Parallel Computing with CUDA\" by John Cheng, Max Grossman, and Ty McKercher.\n",
      "  - \"Programming Massively Parallel Processors\" by David B. Kirk and Wen-mei W. Hwu (a foundational text on GPU computing).\n",
      "\n",
      "- **Online Courses**:\n",
      "  - NVIDIA's own courses on their Deep Learning Institute website.\n",
      "  - Coursera and Udacity offer instructor-led and self-paced courses focusing on CUDA and GPU programming.\n",
      "\n",
      "- **Forums and Communities**:\n",
      "  - CUDA Developer Forums (NVIDIA Developer site).\n",
      "  - Stack Overflow (search for CUDA-related questions).\n",
      "\n",
      "By engaging with these resources and communities, you will find a wealth of knowledge and assistance as you explore CUDA programming and performance optimization. If there's any specific aspect you'd like to explore further, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what does memcpy stands for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: `memcpy` stands for \"memory copy.\" It is a standard library function in C and C++ that is used to copy a specified number of bytes from one memory location to another.\n",
      "\n",
      "### Syntax\n",
      "The typical syntax for `memcpy` is:\n",
      "\n",
      "```c\n",
      "void *memcpy(void *dest, const void *src, size_t n);\n",
      "```\n",
      "\n",
      "### Parameters:\n",
      "- **`dest`**: A pointer to the destination memory location where bytes will be copied to.\n",
      "- **`src`**: A pointer to the source memory location from which bytes will be copied.\n",
      "- **`n`**: The number of bytes to copy.\n",
      "\n",
      "### Return Value:\n",
      "The function returns a pointer to the destination (`dest`).\n",
      "\n",
      "### Example Usage:\n",
      "Here's a simple example of how to use `memcpy`:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "#include <string.h>\n",
      "\n",
      "int main() {\n",
      "    char src[50] = \"Hello, World!\";\n",
      "    char dest[50];\n",
      "\n",
      "    // Copying the string from src to dest\n",
      "    memcpy(dest, src, strlen(src) + 1);\n",
      "\n",
      "    printf(\"Source: %s\\n\", src);\n",
      "    printf(\"Destination: %s\\n\", dest);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Important Notes:\n",
      "- **Safety**: `memcpy` does not perform any bounds checking, so it's your responsibility to ensure that the destination buffer is large enough to hold the copied data to prevent buffer overflows.\n",
      "- **Overlapping Memory**: If the source and destination buffers overlap, the behavior is undefined. For overlapping regions, it is safer to use `memmove`, which handles overlapping memory areas correctly.\n",
      "- **Performance**: `memcpy` can be very efficient for large memory transfers and is often implemented in a highly optimized manner under the hood. When working with device memory in CUDA, `cudaMemcpy` is used instead to manage transfers between the host (CPU) and device (GPU) memory.\n",
      "\n",
      "In the context of CUDA, `cudaMemcpy` is used for similar purposes, to copy data between host and device memory, but it is specific to CUDA and includes additional functionality (like specifying the direction of memory transfer).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  we are not copying anythign tho?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: When discussing `cudaMemcpy` in the context of CUDA programming, it's important to clarify that the purpose of this function is indeed to copy data between different memory spaces. This is crucial for GPU computation, where the CPU (host) and GPU (device) manage their own separate memory.\n",
      "\n",
      "### Context of `cudaMemcpy`\n",
      "\n",
      "In CUDA programming, you generally do the following:\n",
      "\n",
      "1. **Allocate Memory on the Device**: You create memory on the GPU using functions like `cudaMalloc()`.\n",
      "2. **Copy Data from Host to Device**: You typically need to copy data from the host (CPU memory) to the device (GPU memory) so that your CUDA kernels can operate on this data. This is where `cudaMemcpy` comes in.\n",
      "3. **Launch CUDA Kernels**: Once the data is in device memory, you launch kernels that operate on this data.\n",
      "4. **Copy Results Back**: After computation, you copy any results from device memory back to host memory using `cudaMemcpy`.\n",
      "\n",
      "### Example of Data Transfer with `cudaMemcpy`\n",
      "\n",
      "Here’s a more concrete example that illustrates the complete process of copying data between host and device:\n",
      "\n",
      "```cpp\n",
      "#include <stdio.h>\n",
      "\n",
      "// Kernel function to add elements from two arrays\n",
      "__global__ void addArrays(int *a, int *b, int *c, int size) {\n",
      "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "    if (index < size) {\n",
      "        c[index] = a[index] + b[index];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int size = 1024;\n",
      "    int bytes = size * sizeof(int);\n",
      "    \n",
      "    // Allocate memory on the host\n",
      "    int *h_a = (int *)malloc(bytes);\n",
      "    int *h_b = (int *)malloc(bytes);\n",
      "    int *h_c = (int *)malloc(bytes);\n",
      "\n",
      "    // Initialize host arrays\n",
      "    for (int i = 0; i < size; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "\n",
      "    // Allocate memory on the device\n",
      "    int *d_a, *d_b, *d_c;\n",
      "    cudaMalloc(&d_a, bytes);\n",
      "    cudaMalloc(&d_b, bytes);\n",
      "    cudaMalloc(&d_c, bytes);\n",
      "\n",
      "    // Copy data from host to device\n",
      "    cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);\n",
      "\n",
      "    // Launch kernel\n",
      "    int threadsPerBlock = 256;\n",
      "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
      "    addArrays<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, size);\n",
      "\n",
      "    // Copy result from device to host\n",
      "    cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);\n",
      "\n",
      "    // Print some results\n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"h_c[%d] = %d\\n\", i, h_c[i]);\n",
      "    }\n",
      "\n",
      "    // Free memory\n",
      "    free(h_a);\n",
      "    free(h_b);\n",
      "    free(h_c);\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_c);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Breakdown of the Example\n",
      "\n",
      "1. **Host Memory Allocation**: Allocate memory on the host for input arrays `h_a`, `h_b` and output array `h_c`.\n",
      "2. **Initialization**: Initialize the host arrays with values.\n",
      "3. **Device Memory Allocation**: Allocate memory on the device for input and output arrays.\n",
      "4. **Copying Data**: Use `cudaMemcpy` to copy data from host arrays `h_a` and `h_b` to device arrays `d_a` and `d_b`. This is necessary so that the GPU can operate on this data.\n",
      "5. **Kernel Launch**: The kernel `addArrays` is launched to perform computation on the data in device memory.\n",
      "6. **Copying Results**: Use `cudaMemcpy` again to copy the results from the device array `d_c` back to the host array `h_c`.\n",
      "7. **Freeing Memory**: Free the allocated memory on both the host and the device.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Using `cudaMemcpy` is fundamental in CUDA programming to facilitate data transfers between the host and device. This ensures that the device has the data necessary for computation and that results are brought back to the host for further processing or display. If you have further questions about the CUDA memory model or other related topics, just let me know!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  i see, who is the CEO of OpenAI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: As of my last knowledge update in October 2023, the CEO of OpenAI is Sam Altman. He has held this position since March 2019. However, leadership positions can change, so I recommend checking the latest news or the official OpenAI website for the most current information.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what about the director of shining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \"The Shining\" is a classic horror film directed by Stanley Kubrick, released in 1980. The film is based on Stephen King's 1977 novel of the same name. Kubrick is known for his meticulous filmmaking and has directed numerous acclaimed films across various genres. If you have specific questions about the film or its director, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  is messi the goat or Ronaldo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The debate over whether Lionel Messi or Cristiano Ronaldo is the \"GOAT\" (Greatest of All Time) in football (soccer) is one of the most passionate and enduring discussions among fans of the sport. Both players have had extraordinary careers, and each has a strong case for being considered the GOAT. Here are some arguments in favor of each:\n",
      "\n",
      "### Lionel Messi\n",
      "\n",
      "1. **Natural Talent**: Many fans appreciate Messi's extraordinary dribbling skills, vision, and playmaking abilities. His low center of gravity allows him to maneuver through defenders with ease.\n",
      "\n",
      "2. **Club Success**: Messi spent the majority of his career at FC Barcelona, where he won numerous titles, including multiple UEFA Champions League titles and La Liga championships. He holds the record for the most goals in a calendar year and the most goals in La Liga history.\n",
      "\n",
      "3. **Individual Awards**: Messi has won multiple Ballon d'Or awards, which are given annually to the best player in the world.\n",
      "\n",
      "4. **Consistency**: He has maintained a high level of performance over many years, often being the top scorer in his league and in Europe.\n",
      "\n",
      "### Cristiano Ronaldo\n",
      "\n",
      "1. **Physicality and Athleticism**: Ronaldo is known for his incredible physical attributes, including speed, strength, and aerial ability. His fitness regimen allows him to continue performing at a high level into his late 30s.\n",
      "\n",
      "2. **Versatility**: Ronaldo has proven himself in multiple leagues, having had significant success in the Premier League with Manchester United, La Liga with Real Madrid, Serie A with Juventus, and returning to Manchester United, as well as his time with national teams.\n",
      "\n",
      "3. **Goal-Scoring Records**: He is known for his remarkable goal-scoring ability, having scored over 800 official career goals, including a record number of goals in the UEFA Champions League.\n",
      "\n",
      "4. **Individual Awards**: Ronaldo has also won multiple Ballon d'Or awards and is often a favorite for the title due to his impressive statistics and performances.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Ultimately, the question of who the GOAT is can come down to personal preference and what qualities one values more in a football player. Messi often represents creativity and natural talent, while Ronaldo exemplifies hard work, athleticism, and versatility. \n",
      "\n",
      "Both players have had profound impacts on the sport and have inspired countless fans around the world. The debate is likely to continue for years even after both retire. \n",
      "\n",
      "Who do you think is the GOAT?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \n",
    "                                             \"content\": user_input}]},\n",
    "                             config):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"Who are you?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df463f-c2f2-4d99-aea4-dddcc5a26cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
